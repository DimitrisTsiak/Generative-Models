{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8299811,"sourceType":"datasetVersion","datasetId":4930712}],"dockerImageVersionId":30716,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-04T18:59:04.429482Z","iopub.execute_input":"2024-06-04T18:59:04.429864Z","iopub.status.idle":"2024-06-04T18:59:04.446290Z","shell.execute_reply.started":"2024-06-04T18:59:04.429839Z","shell.execute_reply":"2024-06-04T18:59:04.445324Z"},"trusted":true},"execution_count":60,"outputs":[{"name":"stdout","text":"/kaggle/input/harry-potter-books/03 Harry Potter and the Prisoner of Azkaban.txt\n/kaggle/input/harry-potter-books/06 Harry Potter and the Half-Blood Prince.txt\n/kaggle/input/harry-potter-books/05 Harry Potter and the Order of the Phoenix.txt\n/kaggle/input/harry-potter-books/02 Harry Potter and the Chamber of Secrets.txt\n/kaggle/input/harry-potter-books/07 Harry Potter and the Deathly Hallows.txt\n/kaggle/input/harry-potter-books/01 Harry Potter and the Sorcerers Stone.txt\n/kaggle/input/harry-potter-books/04 Harry Potter and the Goblet of Fire.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"#Preprocessing \nimport torch\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import build_vocab_from_iterator\n\nFILE_PATH = \"/kaggle/input/harry-potter-books/\" \n\nbooks_text = []\nfolder_path = FILE_PATH\n\n# Load the text data\nfor file in os.listdir(folder_path):\n    with open(os.path.join(folder_path, file), 'r', encoding='utf-8') as f:\n        books_text.append(f.read())\nprint(books_text[0][:50])\nall_text = \" \".join(books_text)\n#print(all_text[:100])\nall_text = all_text[:500]\n\n\ntokenizer = get_tokenizer('basic_english')\ntokens = tokenizer(all_text)\n#print(tokens[4000:4500])\n\n\nvocabulary = build_vocab_from_iterator([tokens], specials=['<unk>', '<pad>', '<bos>', '<eos>'])\nvocabulary.set_default_index(vocabulary['<unk>'])\nprint(len(vocabulary))\n\n\n\n\nnumericalized_data = torch.tensor(vocabulary(tokens), dtype=torch.long)\nprint(numericalized_data.shape)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-04T19:11:12.826343Z","iopub.execute_input":"2024-06-04T19:11:12.826752Z","iopub.status.idle":"2024-06-04T19:11:12.952011Z","shell.execute_reply.started":"2024-06-04T19:11:12.826723Z","shell.execute_reply":"2024-06-04T19:11:12.951046Z"},"trusted":true},"execution_count":80,"outputs":[{"name":"stdout","text":"Harry Potter was a highly unusual boy in many ways\n77\ntorch.Size([112])\n","output_type":"stream"}]},{"cell_type":"code","source":"#DataLoader\nfrom torch.utils.data import DataLoader, TensorDataset\n\n\nSEQUENCE_LENGTH = 10\nsequence_length = SEQUENCE_LENGTH\nsequences = [numericalized_data[i:i+sequence_length+1] for i in range(len(numericalized_data)-sequence_length)]\n\n# Create a DataLoader\ndataset = TensorDataset(torch.stack(sequences))\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-04T19:11:15.425289Z","iopub.execute_input":"2024-06-04T19:11:15.425638Z","iopub.status.idle":"2024-06-04T19:11:15.432801Z","shell.execute_reply.started":"2024-06-04T19:11:15.425610Z","shell.execute_reply":"2024-06-04T19:11:15.431903Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"#Model Architecture\n\nimport torch.nn as nn\nimport math\n\nclass TransformerModel(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, hidden_dim, dropout):\n        super(TransformerModel, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.pos_encoder = PositionalEncoding(embed_dim, dropout)\n        \n        decoder_layer = nn.TransformerDecoderLayer(embed_dim, num_heads, hidden_dim, dropout)\n        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers)\n        \n        self.fc = nn.Linear(embed_dim, vocab_size)\n        self.embed_dim = embed_dim\n\n    def forward(self, src, tgt, tgt_mask, memory_mask):\n        src = self.embedding(src) * math.sqrt(self.embed_dim)\n        src = self.pos_encoder(src)\n        \n        tgt = self.embedding(tgt) * math.sqrt(self.embed_dim)\n        tgt = self.pos_encoder(tgt)\n        \n        memory = self.transformer_decoder(tgt, src, tgt_mask=tgt_mask, memory_mask=memory_mask)\n        \n        output = self.fc(memory)\n        return output\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, embed_dim, dropout, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        pe = torch.zeros(max_len, embed_dim)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:x.size(0), :]\n        return self.dropout(x)\n\ndef generate_square_subsequent_mask(sz):\n    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n    return mask\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-04T19:11:17.561610Z","iopub.execute_input":"2024-06-04T19:11:17.561953Z","iopub.status.idle":"2024-06-04T19:11:17.576457Z","shell.execute_reply.started":"2024-06-04T19:11:17.561928Z","shell.execute_reply":"2024-06-04T19:11:17.575583Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"import torch.optim as optim\n\n\nVOCAB_SIZE = len(vocabulary)\nEMBED_DIM = 512\nNUM_HEADS = 8\nNUM_LAYERS = 6\nHIDDEN_DIM = 2048\nDROPOUT = 0.0\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = TransformerModel(VOCAB_SIZE, EMBED_DIM, NUM_HEADS, NUM_LAYERS, HIDDEN_DIM, DROPOUT).to(device)\n\n\noptimizer = optim.Adam(model.parameters(), lr=0.001) # try AdamW?\ncriterion = nn.CrossEntropyLoss()\n\n\n\n\n\n\n\ndef train(model, dataloader, optimizer, criterion):\n    model.train()\n    total_loss = 0\n    for batch in dataloader:\n        optimizer.zero_grad()\n        \n        # Get source and target sequences\n        src = batch[0][:, :-1].T.to(device)\n        tgt_input = batch[0][:, :-1].T.to(device)  \n        tgt_output = batch[0][:, 1:].T.to(device)  \n        \n        # Print shapes for debugging\n        #print(f\"src shape: {src.shape}\")\n        #print(f\"tgt_input shape: {tgt_input.shape}\")\n        #print(f\"tgt_output shape: {tgt_output.shape}\")\n        \n        # Generate mask\n        tgt_mask = generate_square_subsequent_mask(tgt_input.size(0)).to(device)\n        \n        # Forward pass\n        output = model(src, tgt_input, tgt_mask, None)\n        \n        # Print output shape for debugging\n        #print(f\"output shape: {output.shape}\")\n        \n        # Compute loss\n        loss = criterion(output.view(-1, VOCAB_SIZE), tgt_output.reshape(-1))\n        \n        # Backpropagation\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    \n    return total_loss / len(dataloader)\n\nN_EPOCHS = 1000\nfor epoch in range(N_EPOCHS):\n    train_loss = train(model, dataloader, optimizer, criterion)\n    print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}')\n","metadata":{"execution":{"iopub.status.busy":"2024-06-04T19:26:35.926122Z","iopub.execute_input":"2024-06-04T19:26:35.926466Z","iopub.status.idle":"2024-06-04T19:28:30.542888Z","shell.execute_reply.started":"2024-06-04T19:26:35.926442Z","shell.execute_reply":"2024-06-04T19:28:30.542024Z"},"trusted":true},"execution_count":93,"outputs":[{"name":"stdout","text":"Epoch 1, Train Loss: 3.9686\nEpoch 2, Train Loss: 3.1534\nEpoch 3, Train Loss: 2.8339\nEpoch 4, Train Loss: 2.6890\nEpoch 5, Train Loss: 2.6826\nEpoch 6, Train Loss: 2.6297\nEpoch 7, Train Loss: 2.6238\nEpoch 8, Train Loss: 2.5615\nEpoch 9, Train Loss: 2.5638\nEpoch 10, Train Loss: 2.5255\nEpoch 11, Train Loss: 2.5328\nEpoch 12, Train Loss: 2.4874\nEpoch 13, Train Loss: 2.5182\nEpoch 14, Train Loss: 2.4879\nEpoch 15, Train Loss: 2.4722\nEpoch 16, Train Loss: 2.4741\nEpoch 17, Train Loss: 2.4669\nEpoch 18, Train Loss: 2.4660\nEpoch 19, Train Loss: 2.4458\nEpoch 20, Train Loss: 2.4526\nEpoch 21, Train Loss: 2.4389\nEpoch 22, Train Loss: 2.4475\nEpoch 23, Train Loss: 2.4320\nEpoch 24, Train Loss: 2.4394\nEpoch 25, Train Loss: 2.4187\nEpoch 26, Train Loss: 2.3937\nEpoch 27, Train Loss: 2.4030\nEpoch 28, Train Loss: 2.4117\nEpoch 29, Train Loss: 2.3839\nEpoch 30, Train Loss: 2.4054\nEpoch 31, Train Loss: 2.3970\nEpoch 32, Train Loss: 2.3907\nEpoch 33, Train Loss: 2.3985\nEpoch 34, Train Loss: 2.3764\nEpoch 35, Train Loss: 2.3921\nEpoch 36, Train Loss: 2.3750\nEpoch 37, Train Loss: 2.3736\nEpoch 38, Train Loss: 2.3750\nEpoch 39, Train Loss: 2.3568\nEpoch 40, Train Loss: 2.3831\nEpoch 41, Train Loss: 2.3755\nEpoch 42, Train Loss: 2.3895\nEpoch 43, Train Loss: 2.3800\nEpoch 44, Train Loss: 2.3638\nEpoch 45, Train Loss: 2.3753\nEpoch 46, Train Loss: 2.3998\nEpoch 47, Train Loss: 2.3851\nEpoch 48, Train Loss: 2.3898\nEpoch 49, Train Loss: 2.3630\nEpoch 50, Train Loss: 2.3873\nEpoch 51, Train Loss: 2.3763\nEpoch 52, Train Loss: 2.3695\nEpoch 53, Train Loss: 2.3685\nEpoch 54, Train Loss: 2.3660\nEpoch 55, Train Loss: 2.3693\nEpoch 56, Train Loss: 2.3620\nEpoch 57, Train Loss: 2.3502\nEpoch 58, Train Loss: 2.3702\nEpoch 59, Train Loss: 2.3527\nEpoch 60, Train Loss: 2.3452\nEpoch 61, Train Loss: 2.3664\nEpoch 62, Train Loss: 2.3683\nEpoch 63, Train Loss: 2.3648\nEpoch 64, Train Loss: 2.3682\nEpoch 65, Train Loss: 2.3700\nEpoch 66, Train Loss: 2.3649\nEpoch 67, Train Loss: 2.3427\nEpoch 68, Train Loss: 2.3605\nEpoch 69, Train Loss: 2.3551\nEpoch 70, Train Loss: 2.3471\nEpoch 71, Train Loss: 2.3520\nEpoch 72, Train Loss: 2.3461\nEpoch 73, Train Loss: 2.3449\nEpoch 74, Train Loss: 2.3509\nEpoch 75, Train Loss: 2.3426\nEpoch 76, Train Loss: 2.3335\nEpoch 77, Train Loss: 2.3442\nEpoch 78, Train Loss: 2.3516\nEpoch 79, Train Loss: 2.3264\nEpoch 80, Train Loss: 2.3322\nEpoch 81, Train Loss: 2.3160\nEpoch 82, Train Loss: 2.3492\nEpoch 83, Train Loss: 2.3321\nEpoch 84, Train Loss: 2.3451\nEpoch 85, Train Loss: 2.3415\nEpoch 86, Train Loss: 2.3491\nEpoch 87, Train Loss: 2.3479\nEpoch 88, Train Loss: 2.3451\nEpoch 89, Train Loss: 2.3614\nEpoch 90, Train Loss: 2.3589\nEpoch 91, Train Loss: 2.3668\nEpoch 92, Train Loss: 2.3796\nEpoch 93, Train Loss: 2.3417\nEpoch 94, Train Loss: 2.3602\nEpoch 95, Train Loss: 2.3529\nEpoch 96, Train Loss: 2.3647\nEpoch 97, Train Loss: 2.3397\nEpoch 98, Train Loss: 2.3569\nEpoch 99, Train Loss: 2.3453\nEpoch 100, Train Loss: 2.3428\nEpoch 101, Train Loss: 2.3399\nEpoch 102, Train Loss: 2.3503\nEpoch 103, Train Loss: 2.3303\nEpoch 104, Train Loss: 2.3317\nEpoch 105, Train Loss: 2.3281\nEpoch 106, Train Loss: 2.3516\nEpoch 107, Train Loss: 2.3361\nEpoch 108, Train Loss: 2.3239\nEpoch 109, Train Loss: 2.3452\nEpoch 110, Train Loss: 2.3331\nEpoch 111, Train Loss: 2.3344\nEpoch 112, Train Loss: 2.3268\nEpoch 113, Train Loss: 2.3382\nEpoch 114, Train Loss: 2.3433\nEpoch 115, Train Loss: 2.3411\nEpoch 116, Train Loss: 2.3465\nEpoch 117, Train Loss: 2.3269\nEpoch 118, Train Loss: 2.3400\nEpoch 119, Train Loss: 2.3507\nEpoch 120, Train Loss: 2.3438\nEpoch 121, Train Loss: 2.3474\nEpoch 122, Train Loss: 2.3389\nEpoch 123, Train Loss: 2.3354\nEpoch 124, Train Loss: 2.3458\nEpoch 125, Train Loss: 2.3487\nEpoch 126, Train Loss: 2.3343\nEpoch 127, Train Loss: 2.3360\nEpoch 128, Train Loss: 2.3224\nEpoch 129, Train Loss: 2.3244\nEpoch 130, Train Loss: 2.3024\nEpoch 131, Train Loss: 2.3227\nEpoch 132, Train Loss: 2.3182\nEpoch 133, Train Loss: 2.3233\nEpoch 134, Train Loss: 2.3325\nEpoch 135, Train Loss: 2.3263\nEpoch 136, Train Loss: 2.3152\nEpoch 137, Train Loss: 2.3119\nEpoch 138, Train Loss: 2.3205\nEpoch 139, Train Loss: 2.3192\nEpoch 140, Train Loss: 2.3200\nEpoch 141, Train Loss: 2.3317\nEpoch 142, Train Loss: 2.3157\nEpoch 143, Train Loss: 2.3187\nEpoch 144, Train Loss: 2.3313\nEpoch 145, Train Loss: 2.3160\nEpoch 146, Train Loss: 2.3433\nEpoch 147, Train Loss: 2.3511\nEpoch 148, Train Loss: 2.3540\nEpoch 149, Train Loss: 2.3420\nEpoch 150, Train Loss: 2.3538\nEpoch 151, Train Loss: 2.3469\nEpoch 152, Train Loss: 2.3183\nEpoch 153, Train Loss: 2.3417\nEpoch 154, Train Loss: 2.3368\nEpoch 155, Train Loss: 2.3356\nEpoch 156, Train Loss: 2.3280\nEpoch 157, Train Loss: 2.3294\nEpoch 158, Train Loss: 2.3244\nEpoch 159, Train Loss: 2.3401\nEpoch 160, Train Loss: 2.3401\nEpoch 161, Train Loss: 2.3409\nEpoch 162, Train Loss: 2.3457\nEpoch 163, Train Loss: 2.3728\nEpoch 164, Train Loss: 2.3457\nEpoch 165, Train Loss: 2.3366\nEpoch 166, Train Loss: 2.3147\nEpoch 167, Train Loss: 2.3378\nEpoch 168, Train Loss: 2.3346\nEpoch 169, Train Loss: 2.3171\nEpoch 170, Train Loss: 2.3330\nEpoch 171, Train Loss: 2.3247\nEpoch 172, Train Loss: 2.3201\nEpoch 173, Train Loss: 2.3275\nEpoch 174, Train Loss: 2.3183\nEpoch 175, Train Loss: 2.3079\nEpoch 176, Train Loss: 2.3241\nEpoch 177, Train Loss: 2.3153\nEpoch 178, Train Loss: 2.3193\nEpoch 179, Train Loss: 2.3167\nEpoch 180, Train Loss: 2.3175\nEpoch 181, Train Loss: 2.3152\nEpoch 182, Train Loss: 2.3190\nEpoch 183, Train Loss: 2.3064\nEpoch 184, Train Loss: 2.3091\nEpoch 185, Train Loss: 2.3176\nEpoch 186, Train Loss: 2.2988\nEpoch 187, Train Loss: 2.3202\nEpoch 188, Train Loss: 2.3107\nEpoch 189, Train Loss: 2.3173\nEpoch 190, Train Loss: 2.3019\nEpoch 191, Train Loss: 2.3220\nEpoch 192, Train Loss: 2.3196\nEpoch 193, Train Loss: 2.3221\nEpoch 194, Train Loss: 2.3093\nEpoch 195, Train Loss: 2.3096\nEpoch 196, Train Loss: 2.3159\nEpoch 197, Train Loss: 2.3208\nEpoch 198, Train Loss: 2.3236\nEpoch 199, Train Loss: 2.3062\nEpoch 200, Train Loss: 2.3342\nEpoch 201, Train Loss: 2.3119\nEpoch 202, Train Loss: 2.3207\nEpoch 203, Train Loss: 2.3237\nEpoch 204, Train Loss: 2.3242\nEpoch 205, Train Loss: 2.3204\nEpoch 206, Train Loss: 2.3214\nEpoch 207, Train Loss: 2.3211\nEpoch 208, Train Loss: 2.3127\nEpoch 209, Train Loss: 2.3105\nEpoch 210, Train Loss: 2.3139\nEpoch 211, Train Loss: 2.3140\nEpoch 212, Train Loss: 2.3247\nEpoch 213, Train Loss: 2.3185\nEpoch 214, Train Loss: 2.3210\nEpoch 215, Train Loss: 2.3413\nEpoch 216, Train Loss: 2.3296\nEpoch 217, Train Loss: 2.3259\nEpoch 218, Train Loss: 2.3381\nEpoch 219, Train Loss: 2.3223\nEpoch 220, Train Loss: 2.3407\nEpoch 221, Train Loss: 2.3288\nEpoch 222, Train Loss: 2.3098\nEpoch 223, Train Loss: 2.3134\nEpoch 224, Train Loss: 2.3253\nEpoch 225, Train Loss: 2.3155\nEpoch 226, Train Loss: 2.3093\nEpoch 227, Train Loss: 2.3133\nEpoch 228, Train Loss: 2.3240\nEpoch 229, Train Loss: 2.3294\nEpoch 230, Train Loss: 2.3056\nEpoch 231, Train Loss: 2.3180\nEpoch 232, Train Loss: 2.3220\nEpoch 233, Train Loss: 2.3053\nEpoch 234, Train Loss: 2.3115\nEpoch 235, Train Loss: 2.3067\nEpoch 236, Train Loss: 2.3125\nEpoch 237, Train Loss: 2.3120\nEpoch 238, Train Loss: 2.2986\nEpoch 239, Train Loss: 2.3112\nEpoch 240, Train Loss: 2.3033\nEpoch 241, Train Loss: 2.3027\nEpoch 242, Train Loss: 2.3009\nEpoch 243, Train Loss: 2.3108\nEpoch 244, Train Loss: 2.3021\nEpoch 245, Train Loss: 2.3082\nEpoch 246, Train Loss: 2.3075\nEpoch 247, Train Loss: 2.3000\nEpoch 248, Train Loss: 2.3250\nEpoch 249, Train Loss: 2.3065\nEpoch 250, Train Loss: 2.3148\nEpoch 251, Train Loss: 2.3289\nEpoch 252, Train Loss: 2.3299\nEpoch 253, Train Loss: 2.3271\nEpoch 254, Train Loss: 2.3214\nEpoch 255, Train Loss: 2.3248\nEpoch 256, Train Loss: 2.3327\nEpoch 257, Train Loss: 2.3266\nEpoch 258, Train Loss: 2.3179\nEpoch 259, Train Loss: 2.3032\nEpoch 260, Train Loss: 2.3149\nEpoch 261, Train Loss: 2.3047\nEpoch 262, Train Loss: 2.3200\nEpoch 263, Train Loss: 2.3259\nEpoch 264, Train Loss: 2.3423\nEpoch 265, Train Loss: 2.3121\nEpoch 266, Train Loss: 2.3191\nEpoch 267, Train Loss: 2.3074\nEpoch 268, Train Loss: 2.3090\nEpoch 269, Train Loss: 2.3141\nEpoch 270, Train Loss: 2.3105\nEpoch 271, Train Loss: 2.3170\nEpoch 272, Train Loss: 2.3170\nEpoch 273, Train Loss: 2.3118\nEpoch 274, Train Loss: 2.3147\nEpoch 275, Train Loss: 2.3215\nEpoch 276, Train Loss: 2.3306\nEpoch 277, Train Loss: 2.3078\nEpoch 278, Train Loss: 2.3119\nEpoch 279, Train Loss: 2.3117\nEpoch 280, Train Loss: 2.3267\nEpoch 281, Train Loss: 2.3292\nEpoch 282, Train Loss: 2.3201\nEpoch 283, Train Loss: 2.3140\nEpoch 284, Train Loss: 2.3101\nEpoch 285, Train Loss: 2.3123\nEpoch 286, Train Loss: 2.3128\nEpoch 287, Train Loss: 2.3014\nEpoch 288, Train Loss: 2.3071\nEpoch 289, Train Loss: 2.3157\nEpoch 290, Train Loss: 2.3059\nEpoch 291, Train Loss: 2.3252\nEpoch 292, Train Loss: 2.3047\nEpoch 293, Train Loss: 2.2937\nEpoch 294, Train Loss: 2.3107\nEpoch 295, Train Loss: 2.2999\nEpoch 296, Train Loss: 2.3133\nEpoch 297, Train Loss: 2.3133\nEpoch 298, Train Loss: 2.3123\nEpoch 299, Train Loss: 2.2954\nEpoch 300, Train Loss: 2.3381\nEpoch 301, Train Loss: 2.3118\nEpoch 302, Train Loss: 2.3110\nEpoch 303, Train Loss: 2.3119\nEpoch 304, Train Loss: 2.3209\nEpoch 305, Train Loss: 2.3447\nEpoch 306, Train Loss: 2.3123\nEpoch 307, Train Loss: 2.3333\nEpoch 308, Train Loss: 2.3356\nEpoch 309, Train Loss: 2.3242\nEpoch 310, Train Loss: 2.3243\nEpoch 311, Train Loss: 2.3170\nEpoch 312, Train Loss: 2.3125\nEpoch 313, Train Loss: 2.3084\nEpoch 314, Train Loss: 2.3371\nEpoch 315, Train Loss: 2.3067\nEpoch 316, Train Loss: 2.3122\nEpoch 317, Train Loss: 2.3199\nEpoch 318, Train Loss: 2.3156\nEpoch 319, Train Loss: 2.3099\nEpoch 320, Train Loss: 2.3115\nEpoch 321, Train Loss: 2.3051\nEpoch 322, Train Loss: 2.3127\nEpoch 323, Train Loss: 2.3064\nEpoch 324, Train Loss: 2.2983\nEpoch 325, Train Loss: 2.3120\nEpoch 326, Train Loss: 2.2948\nEpoch 327, Train Loss: 2.3108\nEpoch 328, Train Loss: 2.3077\nEpoch 329, Train Loss: 2.3064\nEpoch 330, Train Loss: 2.3075\nEpoch 331, Train Loss: 2.3053\nEpoch 332, Train Loss: 2.3173\nEpoch 333, Train Loss: 2.3259\nEpoch 334, Train Loss: 2.3209\nEpoch 335, Train Loss: 2.3170\nEpoch 336, Train Loss: 2.3330\nEpoch 337, Train Loss: 2.3231\nEpoch 338, Train Loss: 2.3212\nEpoch 339, Train Loss: 2.3394\nEpoch 340, Train Loss: 2.3379\nEpoch 341, Train Loss: 2.3598\nEpoch 342, Train Loss: 2.3643\nEpoch 343, Train Loss: 2.3317\nEpoch 344, Train Loss: 2.3522\nEpoch 345, Train Loss: 2.3458\nEpoch 346, Train Loss: 2.3204\nEpoch 347, Train Loss: 2.3266\nEpoch 348, Train Loss: 2.3291\nEpoch 349, Train Loss: 2.3195\nEpoch 350, Train Loss: 2.3207\nEpoch 351, Train Loss: 2.3158\nEpoch 352, Train Loss: 2.3219\nEpoch 353, Train Loss: 2.3131\nEpoch 354, Train Loss: 2.3180\nEpoch 355, Train Loss: 2.3067\nEpoch 356, Train Loss: 2.3211\nEpoch 357, Train Loss: 2.3097\nEpoch 358, Train Loss: 2.3081\nEpoch 359, Train Loss: 2.3125\nEpoch 360, Train Loss: 2.3016\nEpoch 361, Train Loss: 2.3041\nEpoch 362, Train Loss: 2.3092\nEpoch 363, Train Loss: 2.3133\nEpoch 364, Train Loss: 2.3084\nEpoch 365, Train Loss: 2.3044\nEpoch 366, Train Loss: 2.3173\nEpoch 367, Train Loss: 2.3104\nEpoch 368, Train Loss: 2.3307\nEpoch 369, Train Loss: 2.3209\nEpoch 370, Train Loss: 2.3141\nEpoch 371, Train Loss: 2.3245\nEpoch 372, Train Loss: 2.3371\nEpoch 373, Train Loss: 2.3134\nEpoch 374, Train Loss: 2.3418\nEpoch 375, Train Loss: 2.3303\nEpoch 376, Train Loss: 2.3372\nEpoch 377, Train Loss: 2.3451\nEpoch 378, Train Loss: 2.3491\nEpoch 379, Train Loss: 2.3189\nEpoch 380, Train Loss: 2.3311\nEpoch 381, Train Loss: 2.3201\nEpoch 382, Train Loss: 2.3212\nEpoch 383, Train Loss: 2.3206\nEpoch 384, Train Loss: 2.3163\nEpoch 385, Train Loss: 2.3310\nEpoch 386, Train Loss: 2.3189\nEpoch 387, Train Loss: 2.3374\nEpoch 388, Train Loss: 2.3200\nEpoch 389, Train Loss: 2.3391\nEpoch 390, Train Loss: 2.3086\nEpoch 391, Train Loss: 2.3150\nEpoch 392, Train Loss: 2.3086\nEpoch 393, Train Loss: 2.3098\nEpoch 394, Train Loss: 2.3152\nEpoch 395, Train Loss: 2.3138\nEpoch 396, Train Loss: 2.2996\nEpoch 397, Train Loss: 2.3135\nEpoch 398, Train Loss: 2.3179\nEpoch 399, Train Loss: 2.3320\nEpoch 400, Train Loss: 2.3100\nEpoch 401, Train Loss: 2.3140\nEpoch 402, Train Loss: 2.3313\nEpoch 403, Train Loss: 2.3159\nEpoch 404, Train Loss: 2.3034\nEpoch 405, Train Loss: 2.3035\nEpoch 406, Train Loss: 2.3161\nEpoch 407, Train Loss: 2.3161\nEpoch 408, Train Loss: 2.3134\nEpoch 409, Train Loss: 2.3103\nEpoch 410, Train Loss: 2.3044\nEpoch 411, Train Loss: 2.3054\nEpoch 412, Train Loss: 2.3138\nEpoch 413, Train Loss: 2.2998\nEpoch 414, Train Loss: 2.3174\nEpoch 415, Train Loss: 2.2952\nEpoch 416, Train Loss: 2.3090\nEpoch 417, Train Loss: 2.3079\nEpoch 418, Train Loss: 2.3101\nEpoch 419, Train Loss: 2.3084\nEpoch 420, Train Loss: 2.3044\nEpoch 421, Train Loss: 2.3122\nEpoch 422, Train Loss: 2.3133\nEpoch 423, Train Loss: 2.2920\nEpoch 424, Train Loss: 2.3018\nEpoch 425, Train Loss: 2.2980\nEpoch 426, Train Loss: 2.2959\nEpoch 427, Train Loss: 2.3089\nEpoch 428, Train Loss: 2.2967\nEpoch 429, Train Loss: 2.3108\nEpoch 430, Train Loss: 2.3149\nEpoch 431, Train Loss: 2.3143\nEpoch 432, Train Loss: 2.3120\nEpoch 433, Train Loss: 2.3196\nEpoch 434, Train Loss: 2.3096\nEpoch 435, Train Loss: 2.3284\nEpoch 436, Train Loss: 2.3237\nEpoch 437, Train Loss: 2.3286\nEpoch 438, Train Loss: 2.3007\nEpoch 439, Train Loss: 2.3180\nEpoch 440, Train Loss: 2.3048\nEpoch 441, Train Loss: 2.3175\nEpoch 442, Train Loss: 2.3110\nEpoch 443, Train Loss: 2.3029\nEpoch 444, Train Loss: 2.3046\nEpoch 445, Train Loss: 2.3146\nEpoch 446, Train Loss: 2.2984\nEpoch 447, Train Loss: 2.3390\nEpoch 448, Train Loss: 2.3179\nEpoch 449, Train Loss: 2.3110\nEpoch 450, Train Loss: 2.3094\nEpoch 451, Train Loss: 2.3109\nEpoch 452, Train Loss: 2.3100\nEpoch 453, Train Loss: 2.3083\nEpoch 454, Train Loss: 2.3211\nEpoch 455, Train Loss: 2.3051\nEpoch 456, Train Loss: 2.3022\nEpoch 457, Train Loss: 2.3036\nEpoch 458, Train Loss: 2.3139\nEpoch 459, Train Loss: 2.3020\nEpoch 460, Train Loss: 2.2971\nEpoch 461, Train Loss: 2.3047\nEpoch 462, Train Loss: 2.3064\nEpoch 463, Train Loss: 2.3106\nEpoch 464, Train Loss: 2.2898\nEpoch 465, Train Loss: 2.3104\nEpoch 466, Train Loss: 2.3059\nEpoch 467, Train Loss: 2.3070\nEpoch 468, Train Loss: 2.3204\nEpoch 469, Train Loss: 2.3107\nEpoch 470, Train Loss: 2.3039\nEpoch 471, Train Loss: 2.3069\nEpoch 472, Train Loss: 2.2991\nEpoch 473, Train Loss: 2.3008\nEpoch 474, Train Loss: 2.3135\nEpoch 475, Train Loss: 2.3244\nEpoch 476, Train Loss: 2.3046\nEpoch 477, Train Loss: 2.3088\nEpoch 478, Train Loss: 2.3122\nEpoch 479, Train Loss: 2.2969\nEpoch 480, Train Loss: 2.3014\nEpoch 481, Train Loss: 2.3172\nEpoch 482, Train Loss: 2.3095\nEpoch 483, Train Loss: 2.3140\nEpoch 484, Train Loss: 2.3132\nEpoch 485, Train Loss: 2.3051\nEpoch 486, Train Loss: 2.2886\nEpoch 487, Train Loss: 2.2872\nEpoch 488, Train Loss: 2.3047\nEpoch 489, Train Loss: 2.3006\nEpoch 490, Train Loss: 2.3090\nEpoch 491, Train Loss: 2.3099\nEpoch 492, Train Loss: 2.3095\nEpoch 493, Train Loss: 2.3173\nEpoch 494, Train Loss: 2.3148\nEpoch 495, Train Loss: 2.2998\nEpoch 496, Train Loss: 2.2984\nEpoch 497, Train Loss: 2.3081\nEpoch 498, Train Loss: 2.3045\nEpoch 499, Train Loss: 2.3142\nEpoch 500, Train Loss: 2.2962\nEpoch 501, Train Loss: 2.2975\nEpoch 502, Train Loss: 2.2973\nEpoch 503, Train Loss: 2.3048\nEpoch 504, Train Loss: 2.2948\nEpoch 505, Train Loss: 2.3048\nEpoch 506, Train Loss: 2.2992\nEpoch 507, Train Loss: 2.2944\nEpoch 508, Train Loss: 2.2969\nEpoch 509, Train Loss: 2.3043\nEpoch 510, Train Loss: 2.3000\nEpoch 511, Train Loss: 2.3029\nEpoch 512, Train Loss: 2.3100\nEpoch 513, Train Loss: 2.3142\nEpoch 514, Train Loss: 2.3080\nEpoch 515, Train Loss: 2.2986\nEpoch 516, Train Loss: 2.3095\nEpoch 517, Train Loss: 2.3001\nEpoch 518, Train Loss: 2.3159\nEpoch 519, Train Loss: 2.2969\nEpoch 520, Train Loss: 2.3284\nEpoch 521, Train Loss: 2.3093\nEpoch 522, Train Loss: 2.3018\nEpoch 523, Train Loss: 2.3034\nEpoch 524, Train Loss: 2.3215\nEpoch 525, Train Loss: 2.3000\nEpoch 526, Train Loss: 2.3014\nEpoch 527, Train Loss: 2.2871\nEpoch 528, Train Loss: 2.3195\nEpoch 529, Train Loss: 2.2927\nEpoch 530, Train Loss: 2.2989\nEpoch 531, Train Loss: 2.2980\nEpoch 532, Train Loss: 2.3084\nEpoch 533, Train Loss: 2.3043\nEpoch 534, Train Loss: 2.3059\nEpoch 535, Train Loss: 2.3072\nEpoch 536, Train Loss: 2.3120\nEpoch 537, Train Loss: 2.3071\nEpoch 538, Train Loss: 2.3157\nEpoch 539, Train Loss: 2.3040\nEpoch 540, Train Loss: 2.3107\nEpoch 541, Train Loss: 2.3078\nEpoch 542, Train Loss: 2.3048\nEpoch 543, Train Loss: 2.2992\nEpoch 544, Train Loss: 2.3065\nEpoch 545, Train Loss: 2.2914\nEpoch 546, Train Loss: 2.2944\nEpoch 547, Train Loss: 2.3010\nEpoch 548, Train Loss: 2.3018\nEpoch 549, Train Loss: 2.2905\nEpoch 550, Train Loss: 2.3049\nEpoch 551, Train Loss: 2.2959\nEpoch 552, Train Loss: 2.3182\nEpoch 553, Train Loss: 2.3092\nEpoch 554, Train Loss: 2.3136\nEpoch 555, Train Loss: 2.3146\nEpoch 556, Train Loss: 2.2963\nEpoch 557, Train Loss: 2.3169\nEpoch 558, Train Loss: 2.3063\nEpoch 559, Train Loss: 2.3044\nEpoch 560, Train Loss: 2.3229\nEpoch 561, Train Loss: 2.3090\nEpoch 562, Train Loss: 2.3090\nEpoch 563, Train Loss: 2.3135\nEpoch 564, Train Loss: 2.3119\nEpoch 565, Train Loss: 2.3122\nEpoch 566, Train Loss: 2.3068\nEpoch 567, Train Loss: 2.3065\nEpoch 568, Train Loss: 2.3134\nEpoch 569, Train Loss: 2.3206\nEpoch 570, Train Loss: 2.3346\nEpoch 571, Train Loss: 2.3569\nEpoch 572, Train Loss: 2.3926\nEpoch 573, Train Loss: 2.4196\nEpoch 574, Train Loss: 2.3994\nEpoch 575, Train Loss: 2.4997\nEpoch 576, Train Loss: 2.4724\nEpoch 577, Train Loss: 2.5233\nEpoch 578, Train Loss: 2.4695\nEpoch 579, Train Loss: 2.4630\nEpoch 580, Train Loss: 2.4256\nEpoch 581, Train Loss: 2.3985\nEpoch 582, Train Loss: 2.3751\nEpoch 583, Train Loss: 2.4024\nEpoch 584, Train Loss: 2.3791\nEpoch 585, Train Loss: 2.3904\nEpoch 586, Train Loss: 2.3777\nEpoch 587, Train Loss: 2.3929\nEpoch 588, Train Loss: 2.3726\nEpoch 589, Train Loss: 2.3810\nEpoch 590, Train Loss: 2.3705\nEpoch 591, Train Loss: 2.3749\nEpoch 592, Train Loss: 2.3708\nEpoch 593, Train Loss: 2.3568\nEpoch 594, Train Loss: 2.3688\nEpoch 595, Train Loss: 2.3544\nEpoch 596, Train Loss: 2.4343\nEpoch 597, Train Loss: 2.4239\nEpoch 598, Train Loss: 2.4553\nEpoch 599, Train Loss: 2.4402\nEpoch 600, Train Loss: 2.4535\nEpoch 601, Train Loss: 2.4209\nEpoch 602, Train Loss: 2.4331\nEpoch 603, Train Loss: 2.3997\nEpoch 604, Train Loss: 2.4405\nEpoch 605, Train Loss: 2.4139\nEpoch 606, Train Loss: 2.3886\nEpoch 607, Train Loss: 2.3722\nEpoch 608, Train Loss: 2.3791\nEpoch 609, Train Loss: 2.3723\nEpoch 610, Train Loss: 2.3779\nEpoch 611, Train Loss: 2.3658\nEpoch 612, Train Loss: 2.3869\nEpoch 613, Train Loss: 2.3800\nEpoch 614, Train Loss: 2.3719\nEpoch 615, Train Loss: 2.3637\nEpoch 616, Train Loss: 2.3613\nEpoch 617, Train Loss: 2.3606\nEpoch 618, Train Loss: 2.3447\nEpoch 619, Train Loss: 2.3593\nEpoch 620, Train Loss: 2.3516\nEpoch 621, Train Loss: 2.3501\nEpoch 622, Train Loss: 2.3239\nEpoch 623, Train Loss: 2.3562\nEpoch 624, Train Loss: 2.3521\nEpoch 625, Train Loss: 2.3460\nEpoch 626, Train Loss: 2.3479\nEpoch 627, Train Loss: 2.3378\nEpoch 628, Train Loss: 2.3384\nEpoch 629, Train Loss: 2.3363\nEpoch 630, Train Loss: 2.3472\nEpoch 631, Train Loss: 2.3291\nEpoch 632, Train Loss: 2.3211\nEpoch 633, Train Loss: 2.3469\nEpoch 634, Train Loss: 2.3296\nEpoch 635, Train Loss: 2.3506\nEpoch 636, Train Loss: 2.3491\nEpoch 637, Train Loss: 2.3531\nEpoch 638, Train Loss: 2.3395\nEpoch 639, Train Loss: 2.3392\nEpoch 640, Train Loss: 2.3446\nEpoch 641, Train Loss: 2.3375\nEpoch 642, Train Loss: 2.3314\nEpoch 643, Train Loss: 2.3216\nEpoch 644, Train Loss: 2.3203\nEpoch 645, Train Loss: 2.3375\nEpoch 646, Train Loss: 2.3213\nEpoch 647, Train Loss: 2.3471\nEpoch 648, Train Loss: 2.3268\nEpoch 649, Train Loss: 2.3271\nEpoch 650, Train Loss: 2.3218\nEpoch 651, Train Loss: 2.3272\nEpoch 652, Train Loss: 2.3205\nEpoch 653, Train Loss: 2.3236\nEpoch 654, Train Loss: 2.3264\nEpoch 655, Train Loss: 2.3295\nEpoch 656, Train Loss: 2.3451\nEpoch 657, Train Loss: 2.3406\nEpoch 658, Train Loss: 2.3249\nEpoch 659, Train Loss: 2.3242\nEpoch 660, Train Loss: 2.3273\nEpoch 661, Train Loss: 2.3299\nEpoch 662, Train Loss: 2.3388\nEpoch 663, Train Loss: 2.3299\nEpoch 664, Train Loss: 2.3394\nEpoch 665, Train Loss: 2.3267\nEpoch 666, Train Loss: 2.3323\nEpoch 667, Train Loss: 2.3249\nEpoch 668, Train Loss: 2.3399\nEpoch 669, Train Loss: 2.3533\nEpoch 670, Train Loss: 2.3295\nEpoch 671, Train Loss: 2.3255\nEpoch 672, Train Loss: 2.3279\nEpoch 673, Train Loss: 2.3323\nEpoch 674, Train Loss: 2.3459\nEpoch 675, Train Loss: 2.3288\nEpoch 676, Train Loss: 2.3431\nEpoch 677, Train Loss: 2.3211\nEpoch 678, Train Loss: 2.3214\nEpoch 679, Train Loss: 2.3165\nEpoch 680, Train Loss: 2.3361\nEpoch 681, Train Loss: 2.3125\nEpoch 682, Train Loss: 2.3284\nEpoch 683, Train Loss: 2.3284\nEpoch 684, Train Loss: 2.3275\nEpoch 685, Train Loss: 2.3405\nEpoch 686, Train Loss: 2.3194\nEpoch 687, Train Loss: 2.3507\nEpoch 688, Train Loss: 2.3194\nEpoch 689, Train Loss: 2.3399\nEpoch 690, Train Loss: 2.3317\nEpoch 691, Train Loss: 2.3324\nEpoch 692, Train Loss: 2.3492\nEpoch 693, Train Loss: 2.3477\nEpoch 694, Train Loss: 2.3421\nEpoch 695, Train Loss: 2.3313\nEpoch 696, Train Loss: 2.3384\nEpoch 697, Train Loss: 2.3416\nEpoch 698, Train Loss: 2.3337\nEpoch 699, Train Loss: 2.3295\nEpoch 700, Train Loss: 2.3335\nEpoch 701, Train Loss: 2.3155\nEpoch 702, Train Loss: 2.3313\nEpoch 703, Train Loss: 2.3152\nEpoch 704, Train Loss: 2.3375\nEpoch 705, Train Loss: 2.3259\nEpoch 706, Train Loss: 2.3242\nEpoch 707, Train Loss: 2.3157\nEpoch 708, Train Loss: 2.2952\nEpoch 709, Train Loss: 2.3191\nEpoch 710, Train Loss: 2.3229\nEpoch 711, Train Loss: 2.3232\nEpoch 712, Train Loss: 2.3290\nEpoch 713, Train Loss: 2.3279\nEpoch 714, Train Loss: 2.3212\nEpoch 715, Train Loss: 2.3304\nEpoch 716, Train Loss: 2.3141\nEpoch 717, Train Loss: 2.3358\nEpoch 718, Train Loss: 2.3385\nEpoch 719, Train Loss: 2.3233\nEpoch 720, Train Loss: 2.3187\nEpoch 721, Train Loss: 2.3219\nEpoch 722, Train Loss: 2.3193\nEpoch 723, Train Loss: 2.3322\nEpoch 724, Train Loss: 2.3145\nEpoch 725, Train Loss: 2.3150\nEpoch 726, Train Loss: 2.3183\nEpoch 727, Train Loss: 2.3288\nEpoch 728, Train Loss: 2.3160\nEpoch 729, Train Loss: 2.3353\nEpoch 730, Train Loss: 2.3275\nEpoch 731, Train Loss: 2.3198\nEpoch 732, Train Loss: 2.3307\nEpoch 733, Train Loss: 2.3285\nEpoch 734, Train Loss: 2.3203\nEpoch 735, Train Loss: 2.3144\nEpoch 736, Train Loss: 2.3050\nEpoch 737, Train Loss: 2.3291\nEpoch 738, Train Loss: 2.3358\nEpoch 739, Train Loss: 2.3155\nEpoch 740, Train Loss: 2.3226\nEpoch 741, Train Loss: 2.3124\nEpoch 742, Train Loss: 2.3219\nEpoch 743, Train Loss: 2.3146\nEpoch 744, Train Loss: 2.3269\nEpoch 745, Train Loss: 2.3197\nEpoch 746, Train Loss: 2.3315\nEpoch 747, Train Loss: 2.3083\nEpoch 748, Train Loss: 2.3201\nEpoch 749, Train Loss: 2.3188\nEpoch 750, Train Loss: 2.3221\nEpoch 751, Train Loss: 2.3173\nEpoch 752, Train Loss: 2.3181\nEpoch 753, Train Loss: 2.3135\nEpoch 754, Train Loss: 2.3193\nEpoch 755, Train Loss: 2.3066\nEpoch 756, Train Loss: 2.3277\nEpoch 757, Train Loss: 2.3225\nEpoch 758, Train Loss: 2.3230\nEpoch 759, Train Loss: 2.3164\nEpoch 760, Train Loss: 2.3170\nEpoch 761, Train Loss: 2.3217\nEpoch 762, Train Loss: 2.3189\nEpoch 763, Train Loss: 2.3143\nEpoch 764, Train Loss: 2.3231\nEpoch 765, Train Loss: 2.3128\nEpoch 766, Train Loss: 2.3177\nEpoch 767, Train Loss: 2.3068\nEpoch 768, Train Loss: 2.3111\nEpoch 769, Train Loss: 2.3320\nEpoch 770, Train Loss: 2.3249\nEpoch 771, Train Loss: 2.3343\nEpoch 772, Train Loss: 2.3422\nEpoch 773, Train Loss: 2.3453\nEpoch 774, Train Loss: 2.3348\nEpoch 775, Train Loss: 2.3294\nEpoch 776, Train Loss: 2.3196\nEpoch 777, Train Loss: 2.3270\nEpoch 778, Train Loss: 2.3341\nEpoch 779, Train Loss: 2.3268\nEpoch 780, Train Loss: 2.3242\nEpoch 781, Train Loss: 2.3370\nEpoch 782, Train Loss: 2.3341\nEpoch 783, Train Loss: 2.3179\nEpoch 784, Train Loss: 2.3273\nEpoch 785, Train Loss: 2.3211\nEpoch 786, Train Loss: 2.3128\nEpoch 787, Train Loss: 2.3136\nEpoch 788, Train Loss: 2.3159\nEpoch 789, Train Loss: 2.3162\nEpoch 790, Train Loss: 2.3362\nEpoch 791, Train Loss: 2.3090\nEpoch 792, Train Loss: 2.3231\nEpoch 793, Train Loss: 2.3163\nEpoch 794, Train Loss: 2.3223\nEpoch 795, Train Loss: 2.3218\nEpoch 796, Train Loss: 2.3190\nEpoch 797, Train Loss: 2.3281\nEpoch 798, Train Loss: 2.3240\nEpoch 799, Train Loss: 2.3263\nEpoch 800, Train Loss: 2.3114\nEpoch 801, Train Loss: 2.3136\nEpoch 802, Train Loss: 2.3122\nEpoch 803, Train Loss: 2.3185\nEpoch 804, Train Loss: 2.3184\nEpoch 805, Train Loss: 2.3207\nEpoch 806, Train Loss: 2.3308\nEpoch 807, Train Loss: 2.3139\nEpoch 808, Train Loss: 2.3062\nEpoch 809, Train Loss: 2.3188\nEpoch 810, Train Loss: 2.3008\nEpoch 811, Train Loss: 2.3199\nEpoch 812, Train Loss: 2.3137\nEpoch 813, Train Loss: 2.3196\nEpoch 814, Train Loss: 2.3122\nEpoch 815, Train Loss: 2.3258\nEpoch 816, Train Loss: 2.3191\nEpoch 817, Train Loss: 2.3198\nEpoch 818, Train Loss: 2.3288\nEpoch 819, Train Loss: 2.3219\nEpoch 820, Train Loss: 2.3241\nEpoch 821, Train Loss: 2.3269\nEpoch 822, Train Loss: 2.3177\nEpoch 823, Train Loss: 2.3107\nEpoch 824, Train Loss: 2.3380\nEpoch 825, Train Loss: 2.3248\nEpoch 826, Train Loss: 2.3200\nEpoch 827, Train Loss: 2.3191\nEpoch 828, Train Loss: 2.3258\nEpoch 829, Train Loss: 2.3026\nEpoch 830, Train Loss: 2.3323\nEpoch 831, Train Loss: 2.3218\nEpoch 832, Train Loss: 2.3165\nEpoch 833, Train Loss: 2.3179\nEpoch 834, Train Loss: 2.3091\nEpoch 835, Train Loss: 2.3136\nEpoch 836, Train Loss: 2.3039\nEpoch 837, Train Loss: 2.2991\nEpoch 838, Train Loss: 2.3134\nEpoch 839, Train Loss: 2.2999\nEpoch 840, Train Loss: 2.3325\nEpoch 841, Train Loss: 2.3157\nEpoch 842, Train Loss: 2.3180\nEpoch 843, Train Loss: 2.2965\nEpoch 844, Train Loss: 2.3291\nEpoch 845, Train Loss: 2.3090\nEpoch 846, Train Loss: 2.3200\nEpoch 847, Train Loss: 2.3083\nEpoch 848, Train Loss: 2.3109\nEpoch 849, Train Loss: 2.3265\nEpoch 850, Train Loss: 2.3164\nEpoch 851, Train Loss: 2.3357\nEpoch 852, Train Loss: 2.3237\nEpoch 853, Train Loss: 2.3350\nEpoch 854, Train Loss: 2.3471\nEpoch 855, Train Loss: 2.3370\nEpoch 856, Train Loss: 2.3461\nEpoch 857, Train Loss: 2.3253\nEpoch 858, Train Loss: 2.3168\nEpoch 859, Train Loss: 2.3135\nEpoch 860, Train Loss: 2.3253\nEpoch 861, Train Loss: 2.3288\nEpoch 862, Train Loss: 2.3268\nEpoch 863, Train Loss: 2.3244\nEpoch 864, Train Loss: 2.3227\nEpoch 865, Train Loss: 2.3259\nEpoch 866, Train Loss: 2.3241\nEpoch 867, Train Loss: 2.3236\nEpoch 868, Train Loss: 2.3272\nEpoch 869, Train Loss: 2.3278\nEpoch 870, Train Loss: 2.3158\nEpoch 871, Train Loss: 2.3212\nEpoch 872, Train Loss: 2.3286\nEpoch 873, Train Loss: 2.3223\nEpoch 874, Train Loss: 2.3285\nEpoch 875, Train Loss: 2.3175\nEpoch 876, Train Loss: 2.3269\nEpoch 877, Train Loss: 2.3297\nEpoch 878, Train Loss: 2.3369\nEpoch 879, Train Loss: 2.3188\nEpoch 880, Train Loss: 2.3222\nEpoch 881, Train Loss: 2.3340\nEpoch 882, Train Loss: 2.3230\nEpoch 883, Train Loss: 2.3366\nEpoch 884, Train Loss: 2.3364\nEpoch 885, Train Loss: 2.3354\nEpoch 886, Train Loss: 2.3150\nEpoch 887, Train Loss: 2.3159\nEpoch 888, Train Loss: 2.3199\nEpoch 889, Train Loss: 2.3268\nEpoch 890, Train Loss: 2.3227\nEpoch 891, Train Loss: 2.3306\nEpoch 892, Train Loss: 2.3172\nEpoch 893, Train Loss: 2.3224\nEpoch 894, Train Loss: 2.3226\nEpoch 895, Train Loss: 2.3175\nEpoch 896, Train Loss: 2.3054\nEpoch 897, Train Loss: 2.3208\nEpoch 898, Train Loss: 2.3264\nEpoch 899, Train Loss: 2.3182\nEpoch 900, Train Loss: 2.3291\nEpoch 901, Train Loss: 2.3206\nEpoch 902, Train Loss: 2.3336\nEpoch 903, Train Loss: 2.3253\nEpoch 904, Train Loss: 2.3503\nEpoch 905, Train Loss: 2.3084\nEpoch 906, Train Loss: 2.3331\nEpoch 907, Train Loss: 2.3102\nEpoch 908, Train Loss: 2.3156\nEpoch 909, Train Loss: 2.3181\nEpoch 910, Train Loss: 2.3024\nEpoch 911, Train Loss: 2.3233\nEpoch 912, Train Loss: 2.3137\nEpoch 913, Train Loss: 2.3253\nEpoch 914, Train Loss: 2.3241\nEpoch 915, Train Loss: 2.3087\nEpoch 916, Train Loss: 2.3185\nEpoch 917, Train Loss: 2.3196\nEpoch 918, Train Loss: 2.3125\nEpoch 919, Train Loss: 2.3115\nEpoch 920, Train Loss: 2.3250\nEpoch 921, Train Loss: 2.3084\nEpoch 922, Train Loss: 2.2913\nEpoch 923, Train Loss: 2.3106\nEpoch 924, Train Loss: 2.3360\nEpoch 925, Train Loss: 2.3130\nEpoch 926, Train Loss: 2.3271\nEpoch 927, Train Loss: 2.3189\nEpoch 928, Train Loss: 2.3114\nEpoch 929, Train Loss: 2.3216\nEpoch 930, Train Loss: 2.3231\nEpoch 931, Train Loss: 2.3021\nEpoch 932, Train Loss: 2.3241\nEpoch 933, Train Loss: 2.2970\nEpoch 934, Train Loss: 2.3107\nEpoch 935, Train Loss: 2.3207\nEpoch 936, Train Loss: 2.3104\nEpoch 937, Train Loss: 2.3272\nEpoch 938, Train Loss: 2.3276\nEpoch 939, Train Loss: 2.3234\nEpoch 940, Train Loss: 2.3273\nEpoch 941, Train Loss: 2.3201\nEpoch 942, Train Loss: 2.3273\nEpoch 943, Train Loss: 2.3281\nEpoch 944, Train Loss: 2.3259\nEpoch 945, Train Loss: 2.3097\nEpoch 946, Train Loss: 2.3112\nEpoch 947, Train Loss: 2.3195\nEpoch 948, Train Loss: 2.3156\nEpoch 949, Train Loss: 2.3038\nEpoch 950, Train Loss: 2.3256\nEpoch 951, Train Loss: 2.3120\nEpoch 952, Train Loss: 2.3305\nEpoch 953, Train Loss: 2.3142\nEpoch 954, Train Loss: 2.3243\nEpoch 955, Train Loss: 2.3158\nEpoch 956, Train Loss: 2.3146\nEpoch 957, Train Loss: 2.3325\nEpoch 958, Train Loss: 2.3171\nEpoch 959, Train Loss: 2.3085\nEpoch 960, Train Loss: 2.3306\nEpoch 961, Train Loss: 2.3130\nEpoch 962, Train Loss: 2.3332\nEpoch 963, Train Loss: 2.3140\nEpoch 964, Train Loss: 2.3120\nEpoch 965, Train Loss: 2.3267\nEpoch 966, Train Loss: 2.3243\nEpoch 967, Train Loss: 2.3119\nEpoch 968, Train Loss: 2.3362\nEpoch 969, Train Loss: 2.3238\nEpoch 970, Train Loss: 2.3286\nEpoch 971, Train Loss: 2.3300\nEpoch 972, Train Loss: 2.3363\nEpoch 973, Train Loss: 2.3303\nEpoch 974, Train Loss: 2.3322\nEpoch 975, Train Loss: 2.3199\nEpoch 976, Train Loss: 2.3281\nEpoch 977, Train Loss: 2.3332\nEpoch 978, Train Loss: 2.3426\nEpoch 979, Train Loss: 2.3313\nEpoch 980, Train Loss: 2.3141\nEpoch 981, Train Loss: 2.3211\nEpoch 982, Train Loss: 2.3302\nEpoch 983, Train Loss: 2.3229\nEpoch 984, Train Loss: 2.3207\nEpoch 985, Train Loss: 2.3120\nEpoch 986, Train Loss: 2.3301\nEpoch 987, Train Loss: 2.3202\nEpoch 988, Train Loss: 2.3165\nEpoch 989, Train Loss: 2.3147\nEpoch 990, Train Loss: 2.3204\nEpoch 991, Train Loss: 2.3254\nEpoch 992, Train Loss: 2.3276\nEpoch 993, Train Loss: 2.3369\nEpoch 994, Train Loss: 2.3193\nEpoch 995, Train Loss: 2.3311\nEpoch 996, Train Loss: 2.3236\nEpoch 997, Train Loss: 2.3312\nEpoch 998, Train Loss: 2.3332\nEpoch 999, Train Loss: 2.3255\nEpoch 1000, Train Loss: 2.3480\n","output_type":"stream"}]},{"cell_type":"code","source":"\nimport torch.nn.functional as F\n\ndef generate_text(model, tokenizer, vocabulary, seed_text, max_length=50, temperature=1.0):\n    model.eval()\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    \n    tokens = tokenizer(seed_text)\n    numericalized_tokens = [vocabulary[token] for token in tokens]\n    input_tensor = torch.tensor(numericalized_tokens, dtype=torch.long).unsqueeze(1).to(device)\n    \n    generated_tokens = numericalized_tokens\n    \n    with torch.no_grad():\n        for _ in range(max_length):\n            tgt_input = torch.tensor(generated_tokens[-30:], dtype=torch.long).unsqueeze(1).to(device)\n            tgt_mask = generate_square_subsequent_mask(tgt_input.size(0)).to(device)\n            \n            output = model(input_tensor, tgt_input, tgt_mask, None)\n            output = output[-1, 0, :] / temperature  \n            probabilities = F.softmax(output, dim=-1)\n            next_token = torch.multinomial(probabilities, 1).item()\n            \n            generated_tokens.append(next_token)\n            \n            if vocabulary.get_itos()[next_token] == '<eos>':\n                break\n    \n    generated_text = \" \".join([vocabulary.get_itos()[token] for token in generated_tokens])\n    return generated_text\n\n# Example usage:\nseed_text = \"Harry Potter was a highly unusual boy in many ways\"\ngenerated_text = generate_text(model, tokenizer, vocabulary, seed_text, max_length=50, temperature=1.0)\nprint(generated_text)","metadata":{"execution":{"iopub.status.busy":"2024-06-04T19:28:40.544628Z","iopub.execute_input":"2024-06-04T19:28:40.545486Z","iopub.status.idle":"2024-06-04T19:28:40.850906Z","shell.execute_reply.started":"2024-06-04T19:28:40.545442Z","shell.execute_reply":"2024-06-04T19:28:40.849948Z"},"trusted":true},"execution_count":94,"outputs":[{"name":"stdout","text":"harry potter was a highly unusual boy in many ways many potter a a highly potter in in boy . . highly was many boy many many potter many unusual boy potter potter highly many unusual many highly many boy a unusual . unusual highly unusual in unusual potter . unusual in was many was . boy unusual was many\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}